mr:
  metadata:
    name: Llama 2
    spec_urls:
      -
    description: >

    developer: "Meta"
    release_date: "2023"
    model_type: "Large Language Models (LLMs)"
    model_variants:
      - "7 billion parameters"
      - "13 billion parameters"
      - "70 billion parameters"
    architecture: "Optimized Transformer"
    training_data: "A new mix of publicly available online data"
    intended_use: >
      Commercial and research use in English and
      Assistant-like chat for tuned models and diverse natural language generation tasks for pretrained models
    out_of_scope:
      - "Uses that violate applicable laws or regulations"
      - "Uses prohibited by the Acceptable Use Policy and Llama 2 Community License"
      - "Use in languages other than English, unless fine-tuned in compliance with the Llama 2 Community License and Acceptable Use Policy"
  factors: >
    Meta\'s Research Super Cluster and production clusters; third-party cloud compute for fine-tuning, annotation, and evaluation
    539 tCO2eq, 100% offset by Metaâ€™s sustainability program
  metrics:
    performance_benchmarks:
      - "Commonsense Reasoning"
      - "World Knowledge"
      - "Reading Comprehension"
      - "Mathematics"
      - "Multitask Maximal Likelihood Understanding"
      - "BIG-bench Hard"
      - "AGI Evaluation"
    safety_benchmarks:
      - "TruthfulQA (truthful and informative responses)"
      - "Toxigen (toxic generation percentage)"
  evaluation_data:
    source: "Standard academic benchmarks"
  training_data:
    size: "2 trillion tokens"
    freshness: "Cutoff of September 2022 for pretraining data, with some tuning data up to July 2023"
  ethical_considerations:
      - "Model outputs cannot be predicted in advance"
      - "May produce inaccurate, biased, or objectionable responses"
      - "Developers should perform safety testing and tuning specific to their applications before deployment"
  caveats_recommendations:
    - "Not all scenarios have been covered in testing"
    - "Use cases beyond English require fine-tuning within compliance"

  question_sets:
    - fmti2023

  model:
    blackbox_external_model_access: 1
    capabilities_demonstration: 1
    capabilities_description: 0
    centralized_model_documentation: 1
    evaluation_of_capabilities: 1
    external_model_access_protocol: 1
    external_reproducibility_of_capabilities_evaluation: 1
    external_reproducibility_of_intentional_harm_evaluation: 0
    external_reproducibility_of_mitigations_evaluation: 0
    external_reproducibility_of_trustworthiness_evaluation: 0
    external_reproducibility_of_unintentional_harm_evaluation: 1
    full_external_model_access: 1
    inference_compute_evaluation: 0
    inference_duration_evaluation: 1
    input_modality: 1
    intentional_harm_evaluation: 0
    limitations_demonstration: 0
    limitations_description: 1
    mitigations_demonstration: 1
    mitigations_description: 1
    mitigations_evaluation: 1
    model_architecture: 1
    asset_license: 1
    model_components: 1
    model_size: 1
    output_modality: 1
    risks_demonstration: 1
    risks_description: 1
    third_party_capabilities_evaluation: 0
    third_party_evaluation_of_limitations: 1
    third_party_mitigations_evaluation: 0
    third_party_risks_evaluation: 0
    trustworthiness_evaluation: 0
    unintentional_harm_evaluation: 1

  upstream:
    additional_dependencies: 1
    broader_environmental_impact: 0
    carbon_emissions: 1
    compute_hardware: 0
    compute_usage: 0
    core_frameworks: 0
    data_augmentation: 1
    copyrighted_data: 0
    data_creators: 0
    data_curation: 1
    data_license_status: 0
    data_size: 1
    data_source_selection: 0
    data_sources: 0
    development_duration: 1
    direct_external_data_access: 0
    employment_of_data_laborers: 0
    energy_usage: 1
    geographic_distribution_of_data_laborers: 0
    hardware_owner: 1
    harmful_data_filtration: 1
    instructions_for_creating_data: 1
    labor_protections: 0
    mitigations_for_copyright: 0
    mitigations_for_privacy: 1
    model_objectives: 1
    model_stages: 1
    personal_information_in_data: 0
    queryable_external_data_access: 0
    third_party_partners: 0
    use_of_human_labor: 1
    wages: 0
  downstream:
    affected_individuals: 0
    affected_market_sectors: 0
    centralized_documentation_for_downstream_use: 1
    change_log: 1
    deprecation_policy: 1
    distribution_channels: 1
    documentation_for_responsible_downstream_use: 1
    downstream_applications: 1
    feedback_mechanism: 1
    feedback_summary: 0
    geographic_statistics: 0
    government_inquiries: 0
    interoperability_of_usage_and_model_behavior_policies: 0
    justification_for_enforcement_action: 0
    detection_of_machine_generated_content: 0
    model_behavior_policy_enforcement: 0
    monitoring_mechanism: 0
    permitted_restricted_and_prohibited_model_behaviors: 0
    permitted_restricted_and_prohibited_uses: 1
    permitted_and_prohibited_use_of_user_data: 1
    permitted_and_prohibited_users: 1
    products_and_services: 1
    redress_mechanism: 0
    release_decision_making_protocol: 0
    release_process: 1
    terms_of_service: 1
    usage_data_access_protocol: 0
    usage_disclaimers: 1
    usage_policy_enforcement: 0
    usage_policy_violation_appeals_mechanism: 0
    usage_reports: 0
    user_data_protection_policy: 1
    user_interaction_with_ai_system: 1
    versioning_protocol: 1
