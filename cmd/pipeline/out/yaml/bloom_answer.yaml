Additional Information:
- The training supercomputer, Jean Zay, uses mostly nuclear energy, and the heat generated
  is reused for campus heating
- Training estimated to have equivalent of $2-5M in cloud computing, taking place
  in ÃŽle-de-France, France
Caveats and Recommendations:
- Users should be aware of the model's limitations and the potential for generating
  incorrect information
- Indirect users should be informed when the content they're using is created by the
  LLM
- Appropriate age disclaimer or interface should be used based on the content's nature
Ethical Considerations:
- The model may overrepresent some viewpoints and underrepresent others
- Can generate hateful, abusive, or violent language
- Possibility of containing stereotypes or personal information
Evaluation Data:
  Description: Zero-shot evaluations, using the humaneval dataset for Python language
    with metrics such as pass@1, pass@10, pass@100.
Factors:
- Language (e.g., English, Yoruba)
- Domain (e.g., newswire, stories)
- Demographic characteristics (e.g., gender, nationality)
Intended Use:
- Text generation
- Information Extraction
- Question Answering
- Summarization
Metrics:
- Perplexity
- Cross Entropy Loss
Model Details:
  Developed By: BigScience
  Model Type: Transformer-based Language Model
  Name: BigScience Language Open-science Open-access Multilingual (BLOOM) Language
    Model
  Release Date: Monday, 11.July.2022
  Version: 1.0.0
Training Data:
  Description: The BLOOM model was trained on 1.6TB of pre-processed text, converted
    into 350B unique tokens. The training data includes 46 natural languages and 13
    programming languages.
