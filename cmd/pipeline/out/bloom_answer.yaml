caveats_recommentations:
  potential_mitigations:
  - Models trained or fine-tuned downstream should include an updated Model Card.
  - Provide mechanisms for feedback from those affected by the model.
  warnings:
  - Indirect users should be made aware when the content is created by the LLM.
  - Include an age disclaimer or blocking interface as necessary.
ethical_considerations:
  foreseeable_misuse:
  - Spam generation
  - Disinformation and influence operations
  - Disparagement and defamation
  - Harassment and abuse
  - Deception
  - Unconsented impersonation and imitation
  - Unconsented surveillance
evaluation_data:
  description: Evaluation protocols results forthcoming, including zero-shot evaluations
    and train-time evaluations.
  metrics_results:
  - language: python
    pass@1: 0.155
    pass@10: 0.328
    pass@100: 0.572
    task: humaneval
factors:
- Language
- Domain
- Demographic characteristics
intended_use:
  direct_use:
  - Text generation
  - Exploring characteristics of language generated by a language model
  downstream_use:
  - Information Extraction
  - Question Answering
  - Summarization
metrics:
- Perplexity
- Cross Entropy Loss
model_details:
  cite_as: BigScience, _BigScience Language Open-science Open-access Multilingual
    (BLOOM) Language Model_. International, May 2021-May 2022
  developed_by: BigScience
  funded_by:
  - The French government
  - Hugging Face
  - Organizations of contributors
  languages: Multiple (46 natural languages and 13 programming languages)
  license: RAIL License v1.0
  model_type: Transformer-based Language Model
  release_date_estimate: Monday, 11.July.2022
  version: 1.0.0
quantitative_analysis:
  perplexity: 7.045
  training_logs_link: https://huggingface.co/tensorboard/bigscience/tr11-176B-ml-logs/
  training_loss: 1.939
  validation_loss: 2.061
training_data:
  details: Includes 46 natural languages and 13 programming languages.
  size: 1.6TB of pre-processed text, converted into 350B unique tokens
